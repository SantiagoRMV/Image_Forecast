{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento y detección de vacios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vacíos detectados:\n",
      "1. Desde 2023-01-01 12:20:20 hasta 2023-01-01 13:00:20 (3 imágenes faltantes)\n",
      "\n",
      "Has seleccionado llenar el rango desde 2023-01-01 12:20:20 hasta 2023-01-01 13:00:20, con 3 imágenes faltantes.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Parámetros\n",
    "expected_interval = 10  # Intervalo esperado en minutos entre imágenes\n",
    "image_folder = \"../data/Img_test/\"\n",
    "\n",
    "# Listar y ordenar imágenes por timestamp\n",
    "images = []\n",
    "for file in os.listdir(image_folder):\n",
    "    if file.endswith(\".png\"):\n",
    "        try:\n",
    "            # Extraer el timestamp del nombre de la imagen\n",
    "            timestamp_str = file.split(\"_\")[3][1:][:-4]  # Ajustar según el formato real\n",
    "            timestamp = datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")\n",
    "            images.append((file, timestamp))\n",
    "        except (IndexError, ValueError) as e:\n",
    "            print(f\"Advertencia: No se pudo procesar el archivo {file}. Error: {e}\")\n",
    "\n",
    "# Ordenar las imágenes por timestamp\n",
    "images = sorted(images, key=lambda x: x[1])\n",
    "\n",
    "# Detectar vacíos en la secuencia\n",
    "gaps = []\n",
    "for i in range(len(images) - 1):\n",
    "    current_time = images[i][1]\n",
    "    next_time = images[i + 1][1]\n",
    "    delta = (next_time - current_time).total_seconds() / 60  # Diferencia en minutos\n",
    "    if delta > expected_interval:\n",
    "        gaps.append((current_time, next_time))\n",
    "\n",
    "# Mostrar los vacíos detectados o un mensaje si no hay vacíos\n",
    "if not gaps:\n",
    "    print(\"No se detectaron vacíos en la secuencia de imágenes. Todo está completo.\")\n",
    "else:\n",
    "    print(\"Vacíos detectados:\")\n",
    "    gap_info = []\n",
    "    for idx, (start, end) in enumerate(gaps, start=1):\n",
    "        missing_count = int((end - start).total_seconds() / 60 / expected_interval) - 1\n",
    "        gap_info.append((start, end, missing_count))\n",
    "        print(f\"{idx}. Desde {start} hasta {end} ({missing_count} imágenes faltantes)\")\n",
    "\n",
    "    # Preguntar al usuario qué vacío desea llenar\n",
    "    try:\n",
    "        selected_index = int(input(\"\\nSeleccione el índice del vacío que desea llenar: \"))\n",
    "        if selected_index < 1 or selected_index > len(gap_info):\n",
    "            raise ValueError(\"Índice fuera de rango.\")\n",
    "        selected_gap = gap_info[selected_index - 1]\n",
    "        start_gap, end_gap, missing_count = selected_gap\n",
    "        print(f\"\\nHas seleccionado llenar el rango desde {start_gap} hasta {end_gap}, con {missing_count} imágenes faltantes.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}. Por favor, ingrese un número válido.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imágenes seleccionadas como entrada (10):\n",
      "OR_ABI-L2-ACMF-M6_G16_s20230101104020.png\n",
      "OR_ABI-L2-ACMF-M6_G16_s20230101105020.png\n",
      "OR_ABI-L2-ACMF-M6_G16_s20230101110020.png\n",
      "OR_ABI-L2-ACMF-M6_G16_s20230101111020.png\n",
      "OR_ABI-L2-ACMF-M6_G16_s20230101112020.png\n",
      "OR_ABI-L2-ACMF-M6_G16_s20230101113020.png\n",
      "OR_ABI-L2-ACMF-M6_G16_s20230101114020.png\n",
      "OR_ABI-L2-ACMF-M6_G16_s20230101115020.png\n",
      "OR_ABI-L2-ACMF-M6_G16_s20230101120020.png\n",
      "OR_ABI-L2-ACMF-M6_G16_s20230101121020.png\n"
     ]
    }
   ],
   "source": [
    "# Listar y ordenar imágenes por timestamp\n",
    "image_list = []\n",
    "for file in os.listdir(image_folder):\n",
    "    if file.endswith(\".png\"):\n",
    "        # Extraer el timestamp del nombre de la imagen\n",
    "        timestamp_str = file.split(\"_\")[3][1:][:-4]  # Ajustar según el formato de los nombres\n",
    "        timestamp = datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")\n",
    "        image_list.append((file, timestamp))\n",
    "\n",
    "# Ordenar por timestamp\n",
    "image_list = sorted(image_list, key=lambda x: x[1])\n",
    "\n",
    "# Función para obtener las imágenes disponibles antes del vacío\n",
    "def get_images_before_gap(images, gap_start, num_images=10):\n",
    "    \"\"\"\n",
    "    Recupera las imágenes previas al inicio de un vacío.\n",
    "    \n",
    "    Args:\n",
    "        images (list): Lista de imágenes ordenadas por timestamp.\n",
    "        gap_start (datetime): Timestamp del inicio del vacío.\n",
    "        num_images (int): Número de imágenes a recuperar.\n",
    "\n",
    "    Returns:\n",
    "        list: Imágenes seleccionadas previas al vacío.\n",
    "    \"\"\"\n",
    "    # Filtrar imágenes con timestamps anteriores al inicio del vacío\n",
    "    images_before_gap = [img for img in images if img[1] < gap_start]\n",
    "    \n",
    "    # Seleccionar las últimas num_images imágenes\n",
    "    if len(images_before_gap) < num_images:\n",
    "        raise ValueError(f\"No hay suficientes imágenes antes del vacío para seleccionar {num_images}.\")\n",
    "    return images_before_gap[-num_images:]\n",
    "\n",
    "# Recuperar las imágenes previas al vacío seleccionado\n",
    "num_images_input = 10  # Ajusta según el modelo (ej. 5, 10, 20)\n",
    "input_images = get_images_before_gap(image_list, start_gap, num_images=num_images_input)\n",
    "\n",
    "# Mostrar las imágenes seleccionadas\n",
    "print(f\"Imágenes seleccionadas como entrada ({len(input_images)}):\")\n",
    "for img in input_images:\n",
    "    print(img[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre procesar imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imágenes preprocesadas: (10, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Parámetros del preprocesamiento\n",
    "target_size = (64, 64)  # Redimensionar las imágenes a 64x64\n",
    "\n",
    "# Función para preprocesar una imagen\n",
    "def preprocess_image(image_path, target_size):\n",
    "    \"\"\"Carga y preprocesa una imagen.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Ruta a la imagen.\n",
    "        target_size (tuple): Tamaño de salida (ancho, alto).\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Imagen preprocesada.\n",
    "    \"\"\"\n",
    "    # Cargar la imagen en formato RGB\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Redimensionar\n",
    "    image = cv2.resize(image, target_size)\n",
    "    \n",
    "    # Normalizar valores de píxeles (0-255 -> 0-1)\n",
    "    image = image / 255.0\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Cargar y preprocesar las imágenes seleccionadas\n",
    "preprocessed_images = []\n",
    "for img_name, _ in input_images:\n",
    "    img_path = os.path.join(image_folder, img_name)\n",
    "    preprocessed_images.append(preprocess_image(img_path, target_size))\n",
    "\n",
    "# Convertir a un array numpy\n",
    "preprocessed_images = np.array(preprocessed_images)\n",
    "\n",
    "print(f\"Imágenes preprocesadas: {preprocessed_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps faltantes: [datetime.datetime(2023, 1, 1, 12, 30, 20), datetime.datetime(2023, 1, 1, 12, 40, 20), datetime.datetime(2023, 1, 1, 12, 50, 20)]\n",
      "Imágenes faltantes preparadas: (3, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Generar los timestamps faltantes\n",
    "missing_timestamps = [\n",
    "    start_gap + timedelta(minutes=expected_interval * i)\n",
    "    for i in range(1, missing_count + 1)\n",
    "]\n",
    "\n",
    "# Verificar los timestamps generados\n",
    "print(f\"Timestamps faltantes: {missing_timestamps}\")\n",
    "\n",
    "# Crear un placeholder para las imágenes faltantes\n",
    "missing_images = []\n",
    "for ts in missing_timestamps:\n",
    "    # Placeholder con el tamaño 64x64 y 3 canales\n",
    "    missing_images.append(np.zeros((64, 64, 3), dtype=np.float32))\n",
    "\n",
    "# Convertir a un array numpy\n",
    "missing_images = np.array(missing_images)\n",
    "\n",
    "print(f\"Imágenes faltantes preparadas: {missing_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 60, 64, 64, 3)]   0         \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 60, 2, 2, 512)     14714688  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " conv_lstm2d_7 (ConvLSTM2D)  (None, 60, 2, 2, 32)      626816    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 60, 2, 2, 32)      128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv_lstm2d_8 (ConvLSTM2D)  (None, 2, 2, 32)          73856     \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 2, 2, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 12288)             1585152   \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 64, 64, 3)         0         \n",
      "                                                                 \n",
      " lambda_3 (Lambda)           (None, 593, 449, 3)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17000768 (64.85 MB)\n",
      "Trainable params: 17000640 (64.85 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, TimeDistributed, ConvLSTM2D, BatchNormalization, Dense, Reshape, Lambda, Flatten\n",
    ")\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Función para redimensionar imágenes dentro del modelo\n",
    "def resize_output_to_original(x):\n",
    "    return tf.image.resize(x, [593, 449], method=\"bilinear\")\n",
    "\n",
    "# Modelo base VGG16 para extracción de características\n",
    "def build_vgg16_extractor(input_shape=(64, 64, 3)):\n",
    "    vgg = VGG16(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    model = Model(inputs=vgg.input, outputs=vgg.output)  # Mantén la salida convolucional\n",
    "    return model\n",
    "\n",
    "# Crear modelo VGG16 para extracción de características\n",
    "vgg16_extractor = build_vgg16_extractor()\n",
    "\n",
    "# Parámetros del modelo\n",
    "sequence_length = 60  # Tamaño de la secuencia\n",
    "input_shape = (sequence_length, 64, 64, 3)  # Secuencia de `sequence_length` imágenes de 64x64x3\n",
    "\n",
    "# Entrada del modelo\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Extracción de características con VGG16\n",
    "x = TimeDistributed(vgg16_extractor)(input_layer)  # Procesa cada imagen de la secuencia\n",
    "\n",
    "# ConvLSTM para capturar relaciones temporales\n",
    "x = ConvLSTM2D(filters=32, kernel_size=(3, 3), padding=\"same\", return_sequences=True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ConvLSTM2D(filters=32, kernel_size=(3, 3), padding=\"same\", return_sequences=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Decodificación de salida\n",
    "x = Dense(64 * 64 * 3, activation=\"relu\")(Flatten()(x))  # Reconstruir dimensiones\n",
    "x = Reshape((64, 64, 3))(x)  # Reconstruir forma original de la imagen\n",
    "\n",
    "# Redimensionar a tamaño original\n",
    "x = Lambda(resize_output_to_original)(x)\n",
    "\n",
    "# Crear modelo completo\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mse\")\n",
    "\n",
    "# Revisar arquitectura del modelo\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo X_train shape: (20, 60, 64, 64, 3)\n",
      "Nuevo y_train shape: (20, 60, 64, 64, 3)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 1706, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 4 and 60 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_9/lambda_3/resize/ResizeBilinear, IteratorGetNext:1)' with input shapes: [4,593,449,3], [4,60,64,64,3].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimized_model_vgg16_lstm.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     69\u001b[0m     X_train, y_train,\n\u001b[1;32m     70\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[1;32m     71\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,  \u001b[38;5;66;03m# Ajustar según los recursos y el tamaño del dataset\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,  \u001b[38;5;66;03m# Reducir si hay problemas de memoria\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     74\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, model_checkpoint]\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Guardar el modelo final\u001b[39;00m\n\u001b[1;32m     78\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_optimized_model_vgg16_lstm.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/nk/w8qn8x117q3b8_dl48mq47640000gn/T/__autograph_generated_filexpp53x7b.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 1706, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 4 and 60 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_9/lambda_3/resize/ResizeBilinear, IteratorGetNext:1)' with input shapes: [4,593,449,3], [4,60,64,64,3].\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import cv2\n",
    "\n",
    "# Crear X_train y y_train\n",
    "sequence_length = 60  # Tamaño más grande para capturar más contexto\n",
    "image_folder = \"../data/Img_test/\"\n",
    "images = []\n",
    "\n",
    "# Listar y ordenar imágenes por timestamp\n",
    "for file in os.listdir(image_folder):\n",
    "    if file.endswith(\".png\"):\n",
    "        try:\n",
    "            timestamp_str = file.split(\"_\")[3][1:][:-4]  # Extraer timestamp del nombre\n",
    "            timestamp = datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")\n",
    "            images.append((file, timestamp))\n",
    "        except (IndexError, ValueError):\n",
    "            print(f\"Archivo ignorado: {file}\")\n",
    "\n",
    "images = sorted(images, key=lambda x: x[1])  # Ordenar por timestamp\n",
    "\n",
    "# Validar si hay suficientes datos\n",
    "if len(images) < 2 * sequence_length:\n",
    "    raise ValueError(f\"No hay suficientes imágenes para generar secuencias. Se requieren al menos {2 * sequence_length}, pero solo hay {len(images)} disponibles.\")\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(images) - 2 * sequence_length):\n",
    "    current_seq = images[i:i + sequence_length]\n",
    "    next_seq = images[i + sequence_length:i + 2 * sequence_length]\n",
    "    # Verificar si las secuencias son consecutivas (permitir un rango de tolerancia)\n",
    "    time_diff = (next_seq[0][1] - current_seq[-1][1]).total_seconds() / 60\n",
    "    if 9 <= time_diff <= 11:  # Tolerancia de ±1 minuto\n",
    "        # Procesar imágenes para entrada y salida\n",
    "        X = [cv2.resize(cv2.imread(os.path.join(image_folder, img[0])), (64, 64)) for img in current_seq]\n",
    "        y = [cv2.resize(cv2.imread(os.path.join(image_folder, img[0])), (64, 64)) for img in next_seq]\n",
    "        X_train.append(X)\n",
    "        y_train.append(y)\n",
    "\n",
    "# Validar si se generaron datos\n",
    "if not X_train or not y_train:\n",
    "    raise ValueError(\"No se generaron datos para el entrenamiento. Verifica las condiciones de consecutividad o la cantidad de imágenes disponibles.\")\n",
    "\n",
    "# Convertir a arrays numpy\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f\"Nuevo X_train shape: {X_train.shape}\")\n",
    "print(f\"Nuevo y_train shape: {y_train.shape}\")\n",
    "\n",
    "# Dividir los datos en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar los datos\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "y_train = y_train / 255.0\n",
    "y_val = y_val / 255.0\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(\"optimized_model_vgg16_lstm.h5\", save_best_only=True, monitor=\"val_loss\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,  # Ajustar según los recursos y el tamaño del dataset\n",
    "    batch_size=4,  # Reducir si hay problemas de memoria\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Guardar el modelo final\n",
    "model.save(\"final_optimized_model_vgg16_lstm.h5\")\n",
    "print(\"Entrenamiento finalizado. Modelo guardado como 'final_optimized_model_vgg16_lstm.h5'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo entrenado\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_path = \"final_model_vgg16_lstm.h5\"  # Cambia por la ruta correcta si es necesario\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Preprocesar las imágenes seleccionadas para el vacío\n",
    "X_input = np.expand_dims(preprocessed_images, axis=0)  # Agregar dimensión batch\n",
    "\n",
    "# Generar las imágenes faltantes\n",
    "y_pred = model.predict(X_input)\n",
    "\n",
    "print(f\"Imágenes generadas: {y_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "output_folder = \"../data/Images_Forecast\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Guardar las imágenes generadas\n",
    "for pred_image, timestamp in zip(y_pred[0], missing_timestamps):\n",
    "    # Desnormalizar la imagen\n",
    "    pred_image = (pred_image * 255).astype(np.uint8)\n",
    "\n",
    "    # Redimensionar al tamaño original (593x449)\n",
    "    pred_image_resized = cv2.resize(pred_image, (449, 593))\n",
    "\n",
    "    # Crear el nombre del archivo\n",
    "    output_name = f\"OR_ABI-L2-ACMF-M6_G16_s{timestamp.strftime('%Y%m%d%H%M%S')}.png\"\n",
    "    output_path = os.path.join(output_folder, output_name)\n",
    "\n",
    "    # Guardar la imagen\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(pred_image_resized, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"Imagen generada guardada en: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
