{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(folder_path, extensions=None):\n",
    "    \"\"\"\n",
    "    Obtiene todas las rutas de archivos en una carpeta que coincidan con las extensiones especificadas.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Ruta de la carpeta a explorar.\n",
    "        extensions (list[str]): Lista de extensiones de archivo a incluir (e.g., ['.jpg', '.png']).\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Lista de rutas completas de los archivos.\n",
    "    \"\"\"\n",
    "    if extensions is None:\n",
    "        extensions = [\".jpg\"]  # Extensión predeterminada\n",
    "\n",
    "    file_paths = [\n",
    "        os.path.join(folder_path, f)\n",
    "        for f in os.listdir(folder_path)\n",
    "        if any(f.endswith(ext) for ext in extensions)\n",
    "    ]\n",
    "    return file_paths\n",
    "\n",
    "def extract_timestamp(file_path):\n",
    "    \"\"\"\n",
    "    Extrae el timestamp del nombre del archivo.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta completa del archivo.\n",
    "\n",
    "    Returns:\n",
    "        datetime: Timestamp extraído del nombre del archivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        timestamp_str = file_name.split(\"_\")[3][1:15]  # 'sYYYYMMDDHHMMSS'\n",
    "        return datetime.strptime(timestamp_str, \"%Y%m%d%H%M%S\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error al extraer timestamp del archivo {file_path}: {e}\")\n",
    "    \n",
    "def load_images_in_range(folder_path, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Carga imágenes en un rango de fechas especificado.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Ruta de la carpeta que contiene las imágenes.\n",
    "        start_date (datetime): Fecha inicial del rango.\n",
    "        end_date (datetime): Fecha final del rango.\n",
    "\n",
    "    Returns:\n",
    "        list[np.array], list[datetime]: Lista de imágenes y sus timestamps correspondientes.\n",
    "    \"\"\"\n",
    "    files = glob(os.path.join(folder_path, \"*.jpg\"))\n",
    "    images = []\n",
    "    timestamps = []\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            timestamp = extract_timestamp(file)\n",
    "            if start_date <= timestamp <= end_date:\n",
    "                img = cv2.imread(file)\n",
    "                if img is not None:\n",
    "                    images.append(img)\n",
    "                    timestamps.append(timestamp)\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando archivo {file}: {e}\")\n",
    "\n",
    "    # Ordenar por timestamp\n",
    "    sorted_data = sorted(zip(timestamps, images))\n",
    "    if sorted_data:\n",
    "        timestamps, images = zip(*sorted_data)\n",
    "        return list(images), list(timestamps)\n",
    "    return [], []\n",
    "\n",
    "# Preprocesar las imágenes\n",
    "def preprocess_images(images, target_size):\n",
    "    \"\"\"\n",
    "    Redimensiona y normaliza las imágenes.\n",
    "\n",
    "    Args:\n",
    "        images (list[np.array]): Lista de imágenes originales.\n",
    "        target_size (tuple): Dimensiones a las que redimensionar las imágenes.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Imágenes preprocesadas.\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        resized_img = cv2.resize(img, target_size)  # Redimensionar\n",
    "        normalized_img = resized_img / 255.0       # Normalizar a [0, 1]\n",
    "        processed_images.append(normalized_img)\n",
    "    return np.array(processed_images)\n",
    "\n",
    "# Crear ventanas deslizantes\n",
    "def create_sliding_windows(images, sequence_length, output_length):\n",
    "    \"\"\"\n",
    "    Crea pares de entrada y salida usando ventanas deslizantes.\n",
    "\n",
    "    Args:\n",
    "        images (np.array): Imágenes preprocesadas.\n",
    "        sequence_length (int): Longitud de la secuencia de entrada.\n",
    "        output_length (int): Longitud de la secuencia de salida.\n",
    "\n",
    "    Returns:\n",
    "        np.array, np.array: Pares de entrada (X) y salida (y).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(images) - sequence_length - output_length + 1):\n",
    "        X.append(images[i:i + sequence_length])\n",
    "        y.append(images[i + sequence_length:i + sequence_length + output_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def find_gaps(timestamps, interval_minutes=10, tolerance_seconds=10):\n",
    "    \"\"\"\n",
    "    Identifica gaps en los timestamps según un intervalo esperado.\n",
    "\n",
    "    Args:\n",
    "        timestamps (list[datetime]): Lista ordenada de timestamps.\n",
    "        interval_minutes (int): Intervalo esperado entre timestamps (en minutos).\n",
    "\n",
    "    Returns:\n",
    "        list[tuple]: Lista de tuples que contienen (timestamp_anterior, timestamp_actual, delta).\n",
    "    \"\"\"\n",
    "    gaps = []\n",
    "    for i in range(1, len(timestamps)):\n",
    "        delta = timestamps[i] - timestamps[i - 1]\n",
    "        if delta > timedelta(minutes=interval_minutes, seconds=tolerance_seconds):  # Sin tolerancia\n",
    "            gaps.append((timestamps[i - 1], timestamps[i], delta))\n",
    "    return gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 1 gaps.\n",
      "Gap entre 2023-07-07 18:20:21 y 2023-07-10 05:00:20: 2 days, 10:39:59\n",
      "Se cargaron 945 imágenes en el rango de fechas.\n",
      "Primera imagen cargada: 2023-07-01 05:00:21\n",
      "Última imagen cargada: 2023-07-07 18:20:21\n",
      "Secuencias de entrada creadas: (931, 10, 64, 64, 3)\n",
      "Secuencias de salida creadas: (931, 5, 64, 64, 3)\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 10, 64, 64, 3)]   0         \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDi  (None, 10, 2048)          14714688  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 10, 256)           2360320   \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 61440)             7925760   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25197888 (96.12 MB)\n",
      "Trainable params: 10483200 (39.99 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 1706, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 61440 and 3 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_6/dense_10/Sigmoid, IteratorGetNext:1)' with input shapes: [?,61440], [?,5,64,64,3].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImagen guardada: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[41], line 50\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Paso 6: Entrenar el modelo\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X, y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenamiento completado.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Paso 7: Predicción\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Aquí puedes generar imágenes faltantes basadas en una secuencia de entrada proporcionada\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Por ejemplo, tomar las últimas `sequence_length` imágenes para predecir\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/nk/w8qn8x117q3b8_dl48mq47640000gn/T/__autograph_generated_filelywpwp3o.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/santiagoromero/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 1706, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 61440 and 3 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_6/dense_10/Sigmoid, IteratorGetNext:1)' with input shapes: [?,61440], [?,5,64,64,3].\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal para el flujo completo del proyecto:\n",
    "    - Preprocesamiento de datos\n",
    "    - Configuración y entrenamiento del modelo\n",
    "    - Predicción de imágenes faltantes\n",
    "    \"\"\"\n",
    "    # Parámetros de configuración\n",
    "    folder_path = \"../data/Images\"  # Carpeta con las imágenes\n",
    "    extensions = [\".jpg\"]  # Extensiones de archivo válidas\n",
    "    interval_minutes = 10  # Intervalo esperado entre imágenes\n",
    "    target_size = (64, 64)  # Tamaño deseado de las imágenes\n",
    "    sequence_length = 10  # Longitud de la secuencia de entrada\n",
    "    output_length = 5  # Número de imágenes que el modelo debe predecir\n",
    "    start_date = datetime(2023, 7, 1, 5, 0, 0)  # Fecha de inicio\n",
    "    end_date = datetime(2023, 7, 8, 18, 30, 0)  # Fecha de fin\n",
    "\n",
    "    # Paso 1: Obtener rutas de los archivos e identificar gaps\n",
    "    file_paths = get_file_paths(folder_path, extensions)\n",
    "    timestamps = sorted([extract_timestamp(fp) for fp in file_paths])\n",
    "    gaps = find_gaps(timestamps, interval_minutes)\n",
    "    print(f\"Se encontraron {len(gaps)} gaps.\")\n",
    "    for gap in gaps:\n",
    "        print(f\"Gap entre {gap[0]} y {gap[1]}: {gap[2]}\")\n",
    "\n",
    "    # Paso 2: Cargar imágenes en el rango de fechas especificado\n",
    "    images, timestamps_in_range = load_images_in_range(folder_path, start_date, end_date)\n",
    "    print(f\"Se cargaron {len(images)} imágenes en el rango de fechas.\")\n",
    "    if images:\n",
    "        print(f\"Primera imagen cargada: {timestamps_in_range[0]}\")\n",
    "        print(f\"Última imagen cargada: {timestamps_in_range[-1]}\")\n",
    "\n",
    "    # Paso 3: Preprocesar las imágenes (redimensionar y normalizar)\n",
    "    preprocessed_images = preprocess_images(images, target_size)\n",
    "\n",
    "    # Paso 4: Crear ventanas deslizantes para las secuencias de entrenamiento\n",
    "    X, y = create_sliding_windows(preprocessed_images, sequence_length, output_length)\n",
    "    print(f\"Secuencias de entrada creadas: {X.shape}\")\n",
    "    print(f\"Secuencias de salida creadas: {y.shape}\")\n",
    "\n",
    "    # Paso 5: Configurar el modelo basado en VGG16 + LSTM\n",
    "    input_shape = (target_size[0], target_size[1], 3)  # Altura, anchura, canales\n",
    "    model = build_model(input_shape, sequence_length, output_length)\n",
    "\n",
    "    # Visualización del modelo (opcional)\n",
    "    plot_model(model, to_file=\"model_architecture.png\", show_shapes=True)\n",
    "    print(model.summary())\n",
    "\n",
    "    # Paso 6: Entrenar el modelo\n",
    "    history = model.fit(X, y, epochs=10, batch_size=16, validation_split=0.2)\n",
    "    print(\"Entrenamiento completado.\")\n",
    "\n",
    "    # Paso 7: Predicción\n",
    "    # Aquí puedes generar imágenes faltantes basadas en una secuencia de entrada proporcionada\n",
    "    # Por ejemplo, tomar las últimas `sequence_length` imágenes para predecir\n",
    "    test_input = X[-1].reshape(1, sequence_length, *input_shape)  # Usar la última secuencia de entrenamiento como ejemplo\n",
    "    predicted_output = model.predict(test_input)\n",
    "    print(f\"Predicción completada. Forma de la salida: {predicted_output.shape}\")\n",
    "\n",
    "    # Paso 8: Postprocesamiento\n",
    "    # Reconstruir las imágenes predichas desde el formato aplanado a (64, 64, 3)\n",
    "    reconstructed_images = predicted_output.reshape(output_length, target_size[0], target_size[1], 3)\n",
    "    print(f\"Imágenes reconstruidas: {reconstructed_images.shape}\")\n",
    "\n",
    "    # Guardar las imágenes predichas (opcional)\n",
    "    output_folder = \"../data/Predicted_Images\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for idx, img in enumerate(reconstructed_images):\n",
    "        img_path = os.path.join(output_folder, f\"predicted_{idx + 1}.jpg\")\n",
    "        cv2.imwrite(img_path, (img * 255).astype(np.uint8))  # Desnormalizar y guardar\n",
    "        print(f\"Imagen guardada: {img_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuracion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 10, 64, 64, 3)]   0         \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 10, 2048)          14714688  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 10, 256)           2360320   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 61440)             7925760   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25197888 (96.12 MB)\n",
      "Trainable params: 10483200 (39.99 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, Flatten\n",
    "\n",
    "def build_model(input_shape, sequence_length, output_length):\n",
    "    \"\"\"\n",
    "    Construye el modelo basado en VGG16 + LSTM.\n",
    "\n",
    "    El modelo utiliza VGG16 como extractor de características de las imágenes,\n",
    "    seguido por una LSTM para modelar las relaciones temporales en la secuencia.\n",
    "    Finalmente, una capa densa produce la salida, que corresponde a las imágenes generadas.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Dimensiones de las imágenes de entrada (e.g., (64, 64, 3)).\n",
    "        sequence_length (int): Longitud de la secuencia de entrada.\n",
    "        output_length (int): Longitud de la secuencia de salida.\n",
    "\n",
    "    Returns:\n",
    "        Model: Modelo compilado listo para entrenar.\n",
    "    \"\"\"\n",
    "    # Cargar la arquitectura de VGG16 preentrenada en ImageNet, excluyendo las capas superiores (clasificación)\n",
    "    vgg16 = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    # Congelar las capas de VGG16 para evitar que sus pesos se actualicen durante el entrenamiento\n",
    "    for layer in vgg16.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Construir un extractor de características con VGG16 seguido de un Flatten\n",
    "    vgg16_extractor = Sequential([\n",
    "        vgg16,  # Extraer características convolucionales\n",
    "        Flatten()  # Convertir el volumen 3D a un vector 1D\n",
    "    ])\n",
    "\n",
    "    # Capa de entrada: secuencia de imágenes con dimensiones específicas\n",
    "    input_layer = Input(shape=(sequence_length, *input_shape))  # (sequence_length, height, width, channels)\n",
    "\n",
    "    # Extraer características de cada imagen en la secuencia usando TimeDistributed\n",
    "    features = TimeDistributed(vgg16_extractor)(input_layer)  # Salida: (sequence_length, features_dim)\n",
    "\n",
    "    # Modelar relaciones temporales con dos capas LSTM\n",
    "    lstm_output = LSTM(256, return_sequences=True)(features)  # Primera LSTM, salida con secuencias completas\n",
    "    lstm_output = LSTM(128, return_sequences=False)(lstm_output)  # Segunda LSTM, salida final como vector\n",
    "\n",
    "    # Capa densa para producir la salida deseada: las próximas imágenes de la secuencia\n",
    "    output_units = output_length * input_shape[0] * input_shape[1] * input_shape[2]  # Salida flattenizada\n",
    "    output_layer = Dense(output_units, activation=\"sigmoid\")(lstm_output)  # Activación sigmoide para normalización\n",
    "\n",
    "    # Crear y compilar el modelo\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])  # Compilar con optimizador Adam y pérdida MSE\n",
    "\n",
    "    return model\n",
    "\n",
    "# Parámetros del modelo\n",
    "input_shape = (64, 64, 3)  # Dimensiones de las imágenes\n",
    "sequence_length = 10  # Secuencia de entrada (ejemplo)\n",
    "output_length = 5  # Secuencia de salida (predicción)\n",
    "\n",
    "# Crear el modelo\n",
    "model = build_model(input_shape, sequence_length, output_length)\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detalles Clave:\n",
    "\n",
    "1.\tVGG16:\n",
    "- Se carga el modelo preentrenado en ImageNet.\n",
    "- Se eliminan las capas de clasificación finales para usarlo como extractor de características.\n",
    "- Se congela el entrenamiento de las capas preentrenadas.\n",
    "2.\tLSTM:\n",
    "- Recibe secuencias de características extraídas por la VGG16.\n",
    "- Tiene una capa Dense final para predecir las características de la siguiente imagen.\n",
    "3.\tDimensiones:\n",
    "- Asegúrate de que las imágenes de entrada tengan tamaño 64x64 (preprocesamiento).\n",
    "- Define cuántos pasos temporales quieres para la secuencia (time_steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Crear datos de entrenamiento\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m X, y \u001b[38;5;241m=\u001b[39m create_sliding_windows(preprocessed_images, sequence_length, output_length)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[1;32m     23\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X, y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_images' is not defined"
     ]
    }
   ],
   "source": [
    "def create_sliding_windows(images, sequence_length, output_length):\n",
    "    \"\"\"\n",
    "    Crea pares de entrada y salida usando ventanas deslizantes.\n",
    "\n",
    "    Args:\n",
    "        images (np.array): Imágenes preprocesadas.\n",
    "        sequence_length (int): Longitud de la secuencia de entrada.\n",
    "        output_length (int): Longitud de la secuencia de salida.\n",
    "\n",
    "    Returns:\n",
    "        np.array, np.array: Pares de entrada (X) y salida (y).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(images) - sequence_length - output_length + 1):\n",
    "        X.append(images[i:i + sequence_length])\n",
    "        y.append(images[i + sequence_length:i + sequence_length + output_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Crear datos de entrenamiento\n",
    "X, y = create_sliding_windows(preprocessed_images, sequence_length, output_length)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X, y, epochs=10, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediccion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal imágenes cargadas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "# Usar las últimas 10 imágenes para predecir las próximas 5\n",
    "input_sequence = preprocessed_images[-sequence_length:]\n",
    "input_sequence = np.expand_dims(input_sequence, axis=0)  # Expandir dimensiones para el modelo\n",
    "\n",
    "predicted_images = model.predict(input_sequence)\n",
    "\n",
    "# Postprocesamiento: convertir imágenes de [0, 1] a [0, 255]\n",
    "predicted_images = (predicted_images * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la imagen: (449, 593, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Cargar la imagen para verificar sus dimensiones\n",
    "image_path = \"../data/Images/OR_ABI-L2-ACMF-M6_G16_s20230710141020.jpg\"  # Cambia esto por la ruta de la imagen real\n",
    "real_image = cv2.imread(image_path)\n",
    "real_image_shape = real_image.shape  # Altura, Ancho, Canales\n",
    "print(f\"Dimensiones de la imagen: {real_image_shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
